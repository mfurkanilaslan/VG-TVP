<h2>VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting</h2>

<h3><a href="https://aaai.org/aaai-publications/aaai-conference-proceedings/"> VG-TVP Paper </a> :page_with_curl:</h3>
<h3><a href="https://drive.google.com/drive/folders/1-Lka5F-Dh-Fz6CwHDJYjUqieXlt2GCR6?usp=drive_link"> VG-TVP Dataset </a> :book: & :clapper: </h3>
<h3><a href="https://arxiv.org/abs/2412.11621"> Paper from Arxiv </a> :computer: </h3>
<h3><a href="https://twitter.com/muhammetfi"> Twitter </a> :bird: </h3>

<p>
  Thrilled to release Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel LLM-empowered Multimodal Procedural Planning (MPP) framework. It generates cohesive text and video procedural plans given a specified high-level objective. The main challenges are achieving textual and visual informativeness, temporal coherence, and accuracy in procedural plans. VG-TVP leverages the zero-shot reasoning capability of LLMs, the video-to-text generation ability of the video captioning models, and the text-to-video generation ability of diffusion models. VG-TVP improves the interaction between modalities by proposing a novel Fusion of Captioning (FoC) method and using Text-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs to guide the generation of visually-grounded text plans and textual-grounded video plans. To address the scarcity of datasets suitable for MPP, we have curated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We conduct comprehensive experiments and benchmarks to evaluate human preferences (regarding textual and visual informativeness, temporal coherence, and plan accuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP dataset.
  Please check out our paper "GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations"!
</p>
